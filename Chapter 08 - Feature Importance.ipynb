{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as mpl\n",
    "\n",
    "from collections import defaultdict\n",
    "from functools import reduce\n",
    "from path import Path\n",
    "from pprint import pprint\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "%matplotlib inline\n",
    "mpl.style.use('ggplot')\n",
    "mpl.rcParams['figure.figsize'] = 16,6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few interesting notes from this chapter:\n",
    "\n",
    "**Marcos' first law of backtesting:**\n",
    "\n",
    "**Backtesting is not a research tool. Feature importance is.**\n",
    "\n",
    "\n",
    "Once we have found what features are important, we can learn more by conducting a number of experiments.\n",
    "\n",
    "- Are these features important all the time, or only in some specific environments?\n",
    "- What triggers a change in importance over time?\n",
    "- Can these regime switches be predicted?\n",
    "- Are those important features also relevant to other related financial instruments?\n",
    "- Ahe they relevant to other asset classes?\n",
    "- What are the most relevant features across all financial instruments?\n",
    "- What is the subset of features with the  highest rank correlation across the entire investment universe?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "from multiprocess import mpPandasObj\n",
    "from cv import PurgedKFold, cvScore\n",
    "from feature_imp import featImpMDI, featImpMDA, auxFeatImpSFI, getTestData\n",
    "\n",
    "# Code from Chapter 8\n",
    "\n",
    "def featImportance(trnsX, cont, n_estimators=1000, cv=10, max_samples=1.0, numThreads=12, pctEmbargo=0, scoring='accuracy',\n",
    "                   method='SFI', minWLeaf=0.0, **kwargs):\n",
    "    # feature importance from a random forest\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.ensemble import BaggingClassifier\n",
    "    n_jobs = -1 if numThreads > 1 else 1\n",
    "    # 1) prepare classifier, cv. max_features=1 to prevent masking\n",
    "    clf = DecisionTreeClassifier(criterion='entropy', max_features=1, class_weight='balanced', min_weight_fraction_leaf=minWLeaf)\n",
    "    clf = BaggingClassifier(base_estimator=clf, n_estimators=n_estimators, max_features=1, max_samples=max_samples, oob_score=True, n_jobs=n_jobs)\n",
    "    fit = clf.fit(X=trnsX, y=cont['bin'], sample_weight=cont['w'].values)\n",
    "    oob = fit.oob_score_\n",
    "    if method == 'MDI':\n",
    "        imp = featImpMDI(fit, featNames=trnsX.columns, max_features=max_features)\n",
    "        oos = cvScore(clf, X=trnsX, y=cont['bin'], cv=cv, sample_weight=cont['w'], t1=cont['t1'], pctEmbargo=pctEmbargo, scoring=scoring).mean()\n",
    "    elif method == 'MDA':\n",
    "        imp, oos = featImpMDA(clf, X=trnsX, y=cont['bin'], cv=cv, sample_weight=cont['w'], t1=cont['t1'], pctEmbargo=pctEmbargo, scoring=scoring)\n",
    "    elif method == 'SFI':\n",
    "        cvGen = PurgedKFold(n_splits=cv, t1=cont['t1'], pctEmbargo=pctEmbargo)\n",
    "        oos = cvScore(clf, X=trnsX, y=cont['bin'], sample_weight=cont['w'], scoring=scoring, cvGen=cvGen).mean()\n",
    "        clf.n_jobs = 1 # parallelize auxFeatImpSFi rather than clf\n",
    "        imp = mpPandasObj(auxFeatImpSFI, ('featNames', trnsX.columns), numThreads, clf=clf, trnsX=trnsX, cont=cont, scoring=scoring, cvGen=cvGen)\n",
    "\n",
    "    return imp, oob, oos\n",
    "\n",
    "def testFunc(n_features=40, n_informative=10, n_redundant=10, n_estimators=1000, n_samples=10000, cv=10):\n",
    "    # test the importance of the feat importance functions on artificial data\n",
    "    # Nr noise features = n_features - n_informative - n_redundant\n",
    "    trnsX, cont = getTestData(n_features, n_informative, n_redundant, n_samples)\n",
    "    return testDataFunc(trnsX, cont, n_estimators, cv)\n",
    "\n",
    "def testDataFunc(trnsX, cont, n_estimators=1000, cv=10, tag='testFunc', methods=['MDI', 'MDA', 'SFI']):\n",
    "    dict0 = {'minWLeaf': [0.0], 'scoring': ['accuracy'], 'method': methods, 'max_samples':[1.0]}\n",
    "    jobs, out = (dict(zip(dict0, i)) for i in product(*dict0.values())), []\n",
    "    kwargs = {'pathOut': './testFunc/', 'n_estimators': n_estimators, 'tag': tag, 'cv': cv}\n",
    "    for job in jobs:\n",
    "        job['simNum'] = job['method'] + '_' + job['scoring'] + '_' + '%.2f' % job['minWLeaf'] + '_' + str(job['max_samples'])\n",
    "        print(job['simNum'])\n",
    "        kwargs.update(job)\n",
    "        imp, oob, oos = featImportance(trnsX, cont=cont, **kwargs)\n",
    "        plotFeatImportance(imp=imp, oob=oob, oos=oos, **kwargs)\n",
    "        df0 = imp[['mean']] / imp['mean'].abs().sum()\n",
    "        df0['type'] = [i[0] for i in df0.index]\n",
    "        df0 = df0.groupby('type')['mean'].sum().to_dict()\n",
    "        df0.update({'oob': oob, 'oos': oos})\n",
    "        df0.update(job)\n",
    "        out.append(df0)\n",
    "    out = pd.DataFrame(out).sort_values(['method', 'scoring', 'minWLeaf', 'max_samples'])\n",
    "    out = out[['method', 'scoring', 'minWLeaf', 'max_samples', 'I', 'R', 'N', 'oob', 'oos']]\n",
    "    out.to_csv(kwargs['pathOut'] + 'stats.csv')\n",
    "    return\n",
    "\n",
    "def plotFeatImportance(pathOut, imp, oob, oos, method, tag=0, simNum=0, **kwargs):\n",
    "    # plot mean imp bars with std\n",
    "    mpl.figure(figsize=(10, imp.shape[0] / 5.0))\n",
    "    imp = imp.sort_values('mean', ascending=True)\n",
    "    ax = imp['mean'].plot(kind='barh', color='b', alpha=0.25, xerr=imp['std'], error_kw={'ecolor': 'r'})\n",
    "    \n",
    "    if method == 'MDI':\n",
    "        mpl.xlim([0, imp.sum(axis=1).max()])\n",
    "        mpl.axvline(1.0 / imp.shape[0], linewidth=1, color='r', linestyle='dotted')\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    for i, j in zip(ax.patches, imp.index):\n",
    "        ax.text(i.get_width() / 2, i.get_y() + i.get_height() / 2, j, ha='center', va='center', color='black')\n",
    "    mpl.title('tag=' + tag + ' | simNum=' + str(simNum) + ' | oob=' + str(round(oob, 4)) + ' | oos=' + str(round(oos, 4)))\n",
    "    mpl.savefig(pathOut + 'featImportance_' + str(simNum) +'_tag_' + tag + '.png', dpi=100)\n",
    "    mpl.clf()\n",
    "    mpl.close()\n",
    "    return\n",
    "\n",
    "def get_eVec(dot, varThres):\n",
    "    # compute eVec from dot prod matrix, reduce dimension\n",
    "    eVal, eVec = np.linalg.eigh(dot)\n",
    "    idx = eVal.argsort()[::-1]\n",
    "    eVal, eVec = eVal[idx], eVec[:, idx]\n",
    "    \n",
    "    eVal = pd.Series(eVal, index=['PC_' + str(i+1) for i in range(eVal.shape[0])])\n",
    "    eVec = pd.DataFrame(eVec, index=dot.index, columns=eVal.index)\n",
    "    eVec = eVec.loc[:, eVal.index]\n",
    "    \n",
    "    cumVar = eVal.cumsum() / eVal.sum()\n",
    "    dim = cumVar.values.searchsorted(varThres)\n",
    "    eVal, eVec = eVal.iloc[:dim + 1], eVec.iloc[:, :dim + 1]\n",
    "    return eVal, eVec\n",
    "\n",
    "def orthoFeats(dfX, varThres=0.95):\n",
    "    dfZ = dfX.sub(dfX.mean(), axis=1).div(dfX.std(), axis=1)\n",
    "    dot = pd.DataFrame(np.dot(dfZ.T, dfZ), index=dfX.columns, columns=dfX.columns)\n",
    "    eVal, eVec = get_eVec(dot, varThres)\n",
    "    dfP = np.dot(dfZ, eVec)\n",
    "    return dfP\n",
    "\n",
    "# testFunc(n_features=20, n_informative=5, n_redundant=5, n_estimators=100, n_samples=1000, cv=10) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.1a\n",
    "\n",
    "Using the code presented in Section 8.6\n",
    "\n",
    "Generate a dataset $(X, y)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>I_0</th>\n",
       "      <th>I_1</th>\n",
       "      <th>I_2</th>\n",
       "      <th>I_3</th>\n",
       "      <th>R_0</th>\n",
       "      <th>R_1</th>\n",
       "      <th>R_2</th>\n",
       "      <th>R_3</th>\n",
       "      <th>N_0</th>\n",
       "      <th>N_1</th>\n",
       "      <th>N_2</th>\n",
       "      <th>N_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-11-07 22:48:33.686192</th>\n",
       "      <td>-2.689873</td>\n",
       "      <td>0.029924</td>\n",
       "      <td>-3.018511</td>\n",
       "      <td>2.235000</td>\n",
       "      <td>5.249226</td>\n",
       "      <td>1.497503</td>\n",
       "      <td>-4.259974</td>\n",
       "      <td>0.535160</td>\n",
       "      <td>0.497745</td>\n",
       "      <td>0.834478</td>\n",
       "      <td>-0.678012</td>\n",
       "      <td>1.122451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-07 22:49:33.686192</th>\n",
       "      <td>-2.101359</td>\n",
       "      <td>-1.472394</td>\n",
       "      <td>-0.813956</td>\n",
       "      <td>1.586275</td>\n",
       "      <td>2.211585</td>\n",
       "      <td>-0.217206</td>\n",
       "      <td>-3.401780</td>\n",
       "      <td>-0.442523</td>\n",
       "      <td>-1.177745</td>\n",
       "      <td>0.068846</td>\n",
       "      <td>-1.687707</td>\n",
       "      <td>1.116839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-07 22:50:33.686192</th>\n",
       "      <td>-1.999457</td>\n",
       "      <td>-2.212843</td>\n",
       "      <td>-0.398948</td>\n",
       "      <td>1.137350</td>\n",
       "      <td>1.089805</td>\n",
       "      <td>-1.122141</td>\n",
       "      <td>-3.636591</td>\n",
       "      <td>-0.737204</td>\n",
       "      <td>-0.242808</td>\n",
       "      <td>-0.113313</td>\n",
       "      <td>-0.768828</td>\n",
       "      <td>-0.167736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-07 22:51:33.686192</th>\n",
       "      <td>-3.549063</td>\n",
       "      <td>-2.882772</td>\n",
       "      <td>-0.937247</td>\n",
       "      <td>1.138791</td>\n",
       "      <td>1.772129</td>\n",
       "      <td>-2.087764</td>\n",
       "      <td>-5.586152</td>\n",
       "      <td>-0.189952</td>\n",
       "      <td>-1.212815</td>\n",
       "      <td>1.421950</td>\n",
       "      <td>0.729629</td>\n",
       "      <td>0.847081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-07 22:52:33.686192</th>\n",
       "      <td>-0.988358</td>\n",
       "      <td>-0.557092</td>\n",
       "      <td>-1.176953</td>\n",
       "      <td>1.439323</td>\n",
       "      <td>2.270765</td>\n",
       "      <td>0.767248</td>\n",
       "      <td>-2.213849</td>\n",
       "      <td>-0.484104</td>\n",
       "      <td>-0.731081</td>\n",
       "      <td>0.423829</td>\n",
       "      <td>2.072763</td>\n",
       "      <td>0.050671</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 I_0       I_1       I_2       I_3       R_0  \\\n",
       "2019-11-07 22:48:33.686192 -2.689873  0.029924 -3.018511  2.235000  5.249226   \n",
       "2019-11-07 22:49:33.686192 -2.101359 -1.472394 -0.813956  1.586275  2.211585   \n",
       "2019-11-07 22:50:33.686192 -1.999457 -2.212843 -0.398948  1.137350  1.089805   \n",
       "2019-11-07 22:51:33.686192 -3.549063 -2.882772 -0.937247  1.138791  1.772129   \n",
       "2019-11-07 22:52:33.686192 -0.988358 -0.557092 -1.176953  1.439323  2.270765   \n",
       "\n",
       "                                 R_1       R_2       R_3       N_0       N_1  \\\n",
       "2019-11-07 22:48:33.686192  1.497503 -4.259974  0.535160  0.497745  0.834478   \n",
       "2019-11-07 22:49:33.686192 -0.217206 -3.401780 -0.442523 -1.177745  0.068846   \n",
       "2019-11-07 22:50:33.686192 -1.122141 -3.636591 -0.737204 -0.242808 -0.113313   \n",
       "2019-11-07 22:51:33.686192 -2.087764 -5.586152 -0.189952 -1.212815  1.421950   \n",
       "2019-11-07 22:52:33.686192  0.767248 -2.213849 -0.484104 -0.731081  0.423829   \n",
       "\n",
       "                                 N_2       N_3  \n",
       "2019-11-07 22:48:33.686192 -0.678012  1.122451  \n",
       "2019-11-07 22:49:33.686192 -1.687707  1.116839  \n",
       "2019-11-07 22:50:33.686192 -0.768828 -0.167736  \n",
       "2019-11-07 22:51:33.686192  0.729629  0.847081  \n",
       "2019-11-07 22:52:33.686192  2.072763  0.050671  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trnsX, cont = getTestData(n_features=12, n_informative=4, n_redundant=4, n_samples=10000,)\n",
    "trnsX.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.1b\n",
    "\n",
    "Using the code presented in Section 8.6\n",
    "\n",
    "Apply a PCA transformation on X, which we denote $\\dot X$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-2.107580</td>\n",
       "      <td>-1.571988</td>\n",
       "      <td>-3.411484</td>\n",
       "      <td>0.174110</td>\n",
       "      <td>1.646305</td>\n",
       "      <td>0.020611</td>\n",
       "      <td>0.025204</td>\n",
       "      <td>-0.080748</td>\n",
       "      <td>-7.771561e-16</td>\n",
       "      <td>-8.604228e-16</td>\n",
       "      <td>6.661338e-16</td>\n",
       "      <td>-9.992007e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.111479</td>\n",
       "      <td>-0.081332</td>\n",
       "      <td>-1.868084</td>\n",
       "      <td>-1.642531</td>\n",
       "      <td>1.173064</td>\n",
       "      <td>-0.263628</td>\n",
       "      <td>-0.965279</td>\n",
       "      <td>1.162019</td>\n",
       "      <td>-5.551115e-16</td>\n",
       "      <td>-5.481726e-16</td>\n",
       "      <td>7.771561e-16</td>\n",
       "      <td>-2.220446e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.780078</td>\n",
       "      <td>0.752443</td>\n",
       "      <td>-1.502150</td>\n",
       "      <td>-0.493071</td>\n",
       "      <td>0.063027</td>\n",
       "      <td>0.487832</td>\n",
       "      <td>-0.433514</td>\n",
       "      <td>1.375909</td>\n",
       "      <td>-4.440892e-16</td>\n",
       "      <td>-5.100087e-16</td>\n",
       "      <td>5.828671e-16</td>\n",
       "      <td>2.775558e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.866288</td>\n",
       "      <td>1.392241</td>\n",
       "      <td>-2.991825</td>\n",
       "      <td>0.569623</td>\n",
       "      <td>0.373976</td>\n",
       "      <td>-1.732524</td>\n",
       "      <td>-0.786618</td>\n",
       "      <td>1.720167</td>\n",
       "      <td>-8.881784e-16</td>\n",
       "      <td>-1.665335e-16</td>\n",
       "      <td>3.330669e-16</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.169347</td>\n",
       "      <td>-0.742145</td>\n",
       "      <td>-1.238146</td>\n",
       "      <td>0.973626</td>\n",
       "      <td>-1.020943</td>\n",
       "      <td>-1.684008</td>\n",
       "      <td>0.340825</td>\n",
       "      <td>0.260088</td>\n",
       "      <td>-5.551115e-16</td>\n",
       "      <td>-8.187895e-16</td>\n",
       "      <td>4.996004e-16</td>\n",
       "      <td>-6.661338e-16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0 -2.107580 -1.571988 -3.411484  0.174110  1.646305  0.020611  0.025204   \n",
       "1 -1.111479 -0.081332 -1.868084 -1.642531  1.173064 -0.263628 -0.965279   \n",
       "2 -0.780078  0.752443 -1.502150 -0.493071  0.063027  0.487832 -0.433514   \n",
       "3 -0.866288  1.392241 -2.991825  0.569623  0.373976 -1.732524 -0.786618   \n",
       "4 -1.169347 -0.742145 -1.238146  0.973626 -1.020943 -1.684008  0.340825   \n",
       "\n",
       "         7             8             9             10            11  \n",
       "0 -0.080748 -7.771561e-16 -8.604228e-16  6.661338e-16 -9.992007e-16  \n",
       "1  1.162019 -5.551115e-16 -5.481726e-16  7.771561e-16 -2.220446e-16  \n",
       "2  1.375909 -4.440892e-16 -5.100087e-16  5.828671e-16  2.775558e-16  \n",
       "3  1.720167 -8.881784e-16 -1.665335e-16  3.330669e-16  0.000000e+00  \n",
       "4  0.260088 -5.551115e-16 -8.187895e-16  4.996004e-16 -6.661338e-16  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xdot = pd.DataFrame(orthoFeats(trnsX, 1.01))\n",
    "Xdot.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.1c\n",
    "\n",
    "Using the code presented in Section 8.6\n",
    "\n",
    "Compute MDI, MDA, and SFI feature importance on $(\\dot X, y)$, where the base estimator is a RF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xdot = Xdot.set_index(trnsX.index)\n",
    "Xdot.columns = trnsX.columns\n",
    "# testDataFunc(Xdot, cont)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.1d\n",
    "\n",
    "Using the code presented in Section 8.6\n",
    "\n",
    "Do the three methods agree on what features are important? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](testFunc/81c_mdi.png)\n",
    "![title](testFunc/81c_mda.png)\n",
    "![title](testFunc/81c_sfi.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A: While the formatting is slightly screwy on MDI, both MDI and MDA manage to filter out the informative features, though MDI with a higher degree of confidence. SFI strangely puts noise at the top of the list -- albeit with large confidence bands, that if taken into account, also have it with the informative features at the top.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.2a\n",
    "\n",
    "From exercise 1, generate a new dataset $(\\ddot X, y)$, where $\\ddot X$ is a feature union of $X$ and $\\dot X$.\n",
    "\n",
    "Compute MDI, MDA, and SFI feature importance on $(\\ddot X, y)$, where the base estimator is a RF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xdotdot = pd.concat([trnsX, Xdot.add_prefix('Xdot_')], axis=1)\n",
    "# testDataFunc(Xdotdot, cont)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.2b\n",
    "\n",
    "From exercise 1, generate a new dataset $(\\ddot X, y)$, where $\\ddot X$ is a feature union of $X$ and $\\dot X$.\n",
    "\n",
    "Do the three methods agree on what features are important? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](testFunc/82b_mdi.png)\n",
    "![title](testFunc/82b_mda.png)\n",
    "![title](testFunc/82b_sfi.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A: If filtering out results with large confidence bands (N in SFI), then all methods tend rank the original non-transformed informative and redundant features over their PCA-transformed counterparts and those over any noisy ones.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.3a\n",
    "\n",
    "Take the results from exercise 2: \n",
    "\n",
    "Drop the most important features according to each method, resulting in a features matrix $\\dddot X$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_important_features = ['Xdot_I_0', 'Xdot_N_0', 'Xdot_N_1', 'I_2', 'R_2']\n",
    "Xdotdotdot = Xdotdot.loc[:, ~Xdotdot.columns.isin(most_important_features)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.3b\n",
    "\n",
    "Take the results from exercise 2: \n",
    "\n",
    "Compute MDI, MDA, and SFI feature importance on $(\\dddot X, y)$, where the base estimator is a RF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testDataFunc(Xdotdotdot, cont)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](testFunc/83b_mdi.png)\n",
    "![title](testFunc/83b_mda.png)\n",
    "![title](testFunc/83b_sfi.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.3c\n",
    "\n",
    "Take the results from exercise 2: \n",
    "\n",
    "Do you appreciate significant changes in the rankings of important features, relative to the results from exercise 2?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A: MDI seems unperturbed.**\n",
    "\n",
    "**The mean of MDA has dramatically shifted, and it is no longer assigning negative importance to a few informative and redundant features.**\n",
    "\n",
    "**Removing the 2 noisy features at the top from SFI has cleared up the picture a lot, without seemingly affecting the rest.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.4a\n",
    "\n",
    "Using the code presented in Section 8.6:\n",
    "\n",
    "Generate a dataset $(X, y)$ of 1E6 observations, where 5 features are informative, 5 are redundant and 10 are noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trnsX, cont = getTestData(n_features=20, n_informative=5, n_redundant=5, n_samples=int(1e5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.4b\n",
    "\n",
    "Using the code presented in Section 8.6:\n",
    "\n",
    "Split $(X, y)$ into 10 datasets, each of 1E5 observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A: Implemented in thenext answer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.4c\n",
    "\n",
    "Using the code presented in Section 8.6:\n",
    "\n",
    "Compute the parallelized feature importance on each of the 10 datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_of_dfs(dfs):\n",
    "    # I'm confident there's a nicer way to do this...\n",
    "    return reduce(lambda left, right: left.add(right), dfs) / len(dfs)\n",
    "    \n",
    "\n",
    "def testDataFuncN(trnsX, cont, n_estimators=1000, cv=10, n_chunks=10, tag='testFunc'):\n",
    "    dict0 = {'minWLeaf': [0.0], 'scoring': ['accuracy'], 'method': ['MDI'], 'max_samples':[1.0]} # 'SFI' 'MDA',\n",
    "    jobs, out = (dict(zip(dict0, i)) for i in product(*dict0.values())), []\n",
    "    kwargs = {'pathOut': './testFunc/', 'n_estimators': n_estimators, 'tag':tag, 'cv':cv}\n",
    "    for job in jobs:\n",
    "        job['simNum'] = job['method'] + '_' + job['scoring'] + '_' + '%.2f' % job['minWLeaf'] + '_' + str(job['max_samples'])\n",
    "        print(job['simNum'])\n",
    "        kwargs.update(job)\n",
    "        imps, oobs, ooss = [], [], []\n",
    "        for i, chunk in enumerate(np.array_split(trnsX.index, n_chunks)):\n",
    "            trns_chunk = trnsX[trnsX.index.isin(chunk)]\n",
    "            cont_chunk = cont[cont.index.isin(chunk)]\n",
    "            imp, oob, oos = featImportance(trns_chunk, cont=cont_chunk, **kwargs)\n",
    "            imps.append(imp)\n",
    "            oobs.append(oob)\n",
    "            ooss.append(oos)\n",
    "        imp, oob, oos = mean_of_dfs(imps), pd.Series(oobs).mean(), pd.Series(ooss).mean()\n",
    "        plotFeatImportance(imp=imp, oob=oob, oos=oos, **kwargs)\n",
    "        df0 = imp[['mean']] / imp['mean'].abs().sum()\n",
    "        df0['type'] = [i[0] for i in df0.index]\n",
    "        df0 = df0.groupby('type')['mean'].sum().to_dict()\n",
    "        df0.update({'oob': oob, 'oos': oos})\n",
    "        df0.update(job)\n",
    "        out.append(df0)\n",
    "    out = pd.DataFrame(out).sort_values(['method', 'scoring', 'minWLeaf', 'max_samples'])\n",
    "    out = out[['method', 'scoring', 'minWLeaf', 'max_samples', 'I', 'R', 'N', 'oob', 'oos']]\n",
    "    out.to_csv(kwargs['pathOut'] + 'stats.csv')\n",
    "    return\n",
    "\n",
    "# testDataFuncN(trnsX, cont, n_chunks=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parallelized feature importance:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](testFunc/84d_mdi_10chunks.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.4d\n",
    "\n",
    "Using the code presented in Section 8.6:\n",
    "\n",
    "Compute the stacked feature importance on the combined dataset $(X, y)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MDI_accuracy_0.00_1.0\n",
      "MDA_accuracy_0.00_1.0\n"
     ]
    }
   ],
   "source": [
    "testDataFuncN(trnsX, cont, n_chunks=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stacked feature importance:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](testFunc/84d_mdi_1chunk.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.4e \n",
    "\n",
    "Using the code presented in Section 8.6:\n",
    "\n",
    "What causes the discrepancy between the two? Which one is more reliable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A: In terms of ranking informative and redundant features above noisy ones, both methods achieve the same result, while the more computationally intensive (stacked) does so by a much wider margin.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.5\n",
    "\n",
    "Repeat all MDI calculations from exercises 1-4, but this time allow for masking effects. That means, do not set `max_features=int(1)` in Snippet 8.2. How do results differ as a consequence of this change? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MDI_accuracy_0.00_1.0\n",
      "MDI_accuracy_0.00_1.0\n"
     ]
    }
   ],
   "source": [
    "# The code above and in feature_imp.py was temporarily modified to incorporate these and then used to generate the figures below\n",
    "trnsX, cont = getTestData(n_features=12, n_informative=4, n_redundant=4, n_samples=10000,)\n",
    "Xdot = pd.DataFrame(orthoFeats(trnsX, 1.01))\n",
    "Xdot = Xdot.set_index(trnsX.index)\n",
    "Xdot.columns = trnsX.columns\n",
    "# testDataFunc(Xdot, cont, tag='85_1')\n",
    "Xdotdot = pd.concat([trnsX, Xdot.add_prefix('Xdot_')], axis=1)\n",
    "# testDataFunc(Xdotdot, cont, tag='85_2')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](testFunc/85_1.png)\n",
    "![title](testFunc/85_2.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A: There is little change for the PCA-transformed features, while MDI seems to perform a lot better on the union of transformed and non-transformed features when allowing for masking effects.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MDI_accuracy_0.00_1.0\n"
     ]
    }
   ],
   "source": [
    "most_important_features = ['Xdot_I_0', 'Xdot_N_0', 'Xdot_N_1', 'I_2', 'R_2', 'R_0']\n",
    "Xdotdotdot = Xdotdot.loc[:, ~Xdotdot.columns.isin(most_important_features)]\n",
    "testDataFunc(Xdotdotdot, cont, tag='85_3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](testFunc/85_3.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MDI_accuracy_0.00_1.0\n",
      "MDI_accuracy_0.00_1.0\n"
     ]
    }
   ],
   "source": [
    "trnsX, cont = getTestData(n_features=20, n_informative=5, n_redundant=5, n_samples=int(1e5))\n",
    "testDataFuncN(trnsX, cont, n_chunks=10, tag='85_4')\n",
    "testDataFuncN(trnsX, cont, n_chunks=1, tag='85_5')\n",
    "\n",
    "# way faster to do in chunks (10x likely)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parallelized feature importance:\n",
    "![title](testFunc/85_4.png)\n",
    "\n",
    "#### Stacked feature importance:\n",
    "![title](testFunc/85_5.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A: It appears parallelized feature importance completely falls apart when allowing for masking effects, while the stacked method remains solid.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
